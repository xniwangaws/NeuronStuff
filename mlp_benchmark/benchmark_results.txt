================================================================================
MLP Kernel Performance Comparison Results
================================================================================
Platform: trn2
Date: 2026-01-26
Test scripts: benchmark_mlp_step1_generate_neff.py, benchmark_mlp_step2_profile.py

================================================================================
1. Test Results Summary
================================================================================

1.1 Torch Execution Time (includes torch-xla dispatch overhead)
--------------------------------------------------------------------------------
Kernel       Config                         Torch Time (ms)   Notes
--------------------------------------------------------------------------------
nkilib       b=1, s=128, h=1024, i=512      4039.86          Recompiles every time!
neuronxcc    b=1, s=128, h=1024, i=512      1.41             Uses cache
nkilib       b=1, s=256, h=2048, i=1024     4098.30          Recompiles every time!
neuronxcc    b=1, s=256, h=2048, i=1024     1.69             Uses cache

1.2 Actual Kernel Execution Time (measured via neuron-profile)
--------------------------------------------------------------------------------
Kernel       Config                         Kernel Time (μs)  Compile Time (ms)
--------------------------------------------------------------------------------
nkilib       b=1, s=128, h=1024, i=512      45.26            4312

Note: neuronxcc NEFF is compiled via torch-xla with different structure,
      cannot be directly measured with neuron-profile.
      However, actual kernel time should be shorter based on torch execution time.

1.3 Key Findings
--------------------------------------------------------------------------------
- nkilib actual kernel execution time: ~45 microseconds (0.045 ms)
- neuronxcc torch execution time: ~1.4 ms (includes dispatch overhead)
- nkilib torch execution time: ~4000 ms (includes repeated compilation!)

Conclusion: nkilib kernel itself performs well, the issue is @nki.jit recompiles every time.

================================================================================
2. How to Use neuron-profile to Get Kernel Execution Time
================================================================================

Step 1: Compile Kernel to Generate NEFF File
----------------------------------------
Use compile_klir_to_neff() or test framework compilation flow:

```python
from nki.compiler.backends.neuron.TraceKernel import TraceKernel as KLIRTraceKernel
from neuronxcc.starfish.penguin.ir.IRBuilder import IRBuilder
from neuronxcc.starfish.penguin.ir.IRWriter import IRWriter

# 1. Create IR builder
builder = IRBuilder()

# 2. Add input tensors
for name, tensor in inputs.items():
    tensor_ir = builder.tensor(shape=tensor.shape, dtype=tensor.dtype, name=name)

# 3. Trace kernel
traced_kernel = KLIRTraceKernel.trace(func=mlp, platform_target="trn2")
traced_kernel[lnc_count](**inputs)

# 4. Generate penguin.py
IRWriter.run(cu=builder.cu, output=file, save_weights=True, weights_dir=output_dir)

# 5. Compile to generate NEFF
neuronx-cc compile --framework XLA penguin.py --target trn2 --lnc 2
# Output: file.neff
```

Step 2: Save Input Data as .bin Files
----------------------------------------
```python
for name, tensor in inputs.items():
    if isinstance(tensor, np.ndarray):
        with open(f"inp-{name}-000.bin", 'wb') as f:
            f.write(tensor.tobytes())
```

Step 3: Execute with neuron-profile capture
----------------------------------------
```bash
neuron-profile capture \
    --save-output \
    --neff file.neff \
    --num-exec=10 \
    hidden_tensor inp-hidden_tensor-000.bin \
    gate_proj_weights_tensor inp-gate_proj_weights_tensor-000.bin \
    up_proj_weights_tensor inp-up_proj_weights_tensor-000.bin \
    down_proj_weights_tensor inp-down_proj_weights_tensor-000.bin
```

Output: profile.ntff, out, out.2, out.3, ...

Step 4: View Execution Time Summary
----------------------------------------
```bash
neuron-profile show-session -s profile.ntff
```

Example output:
```
+---------+----------------+-----------+------+-------+----------+--------+--------+
| NODE ID | EXEC TIME (NS) | EXECUTOR  | NAME | TRACE |  CYCLES  | EVENTS | ERRORS |
+---------+----------------+-----------+------+-------+----------+--------+--------+
|       0 |              0 | ND 0 NC 4 | sg00 |   873 | 42002434 |    513 |      0 |
|         |                | ND 0 NC 5 | sg01 |   873 | 45256596 |    512 |      0 |
+---------+----------------+-----------+------+-------+----------+--------+--------+
```

Step 5: Get Detailed Performance Data (JSON format)
----------------------------------------
```bash
neuron-profile view \
    -n file.neff \
    -s profile.ntff \
    --output-format=json \
    --output-file=profile.json
```

Parse JSON to get actual execution time:
```python
import json
with open('profile.json') as f:
    data = json.load(f)

# Get total execution time
total_time = data['summary'][0]['total_time']  # seconds
print(f"Kernel execution time: {total_time * 1e6:.2f} μs")

# Other useful metrics
print(f"Hardware FLOPS: {data['summary'][0]['hardware_flops']}")
print(f"Matmul instructions: {data['summary'][0]['matmul_instruction_count']}")
print(f"Tensor engine active: {data['summary'][0]['tensor_engine_active_time'] * 1e6:.2f} μs")
print(f"DMA active: {data['summary'][0]['dma_active_time'] * 1e6:.2f} μs")
```

================================================================================
3. Detailed Performance Data (nkilib MLP, b=1, s=128, h=1024, i=512)
================================================================================

Data extracted from profile.json:

Basic metrics:
- total_time: 45.26 μs (4.5256596e-05 seconds)
- neuroncore_cycle_count: 54307 cycles
- hardware_flops: 427,819,008
- matmul_instruction_count: 72
- event_count: 1025
- trace_count: 3458

Engine active time:
- tensor_engine_active_time: 20.70 μs (45.74%)
- dma_active_time: 20.46 μs (45.20%)
- vector_engine_active_time: 9.38 μs (20.72%)
- scalar_engine_active_time: 6.53 μs (14.43%)
- gpsimd_engine_active_time: 4.74 μs (10.47%)
- sync_engine_active_time: 3.92 μs (8.66%)

Memory metrics:
- hbm_read_bytes: 6,553,600 (6.25 MB)
- hbm_write_bytes: 262,144 (0.25 MB)
- inputs_outputs_weights_size_bytes: 3,670,016 (3.5 MB)
- sbuf_read_bytes: 326,656
- sbuf_write_bytes: 6,564,864

Efficiency metrics:
- mfu_estimated_percent: 5.66%
- mfu_max_achievable_estimated_percent: 26.89%
- mm_arithmetic_intensity: 59.08

================================================================================
4. Root Cause Analysis
================================================================================

Problem: nkilib using @nki.jit recompiles on every xm.mark_step() call

Evidence:
1. Logs show "Compiler status PASS" on every iteration
2. neuronxcc logs show "Using a cached neff at /var/tmp/neuron-compile-cache/..."
3. nkilib execution time ~4000ms ≈ compile time (~4s) + execution time (~0.045ms)

Possible causes:
1. nkilib generates HLO graph with minor differences each time (e.g., tensor IDs), causing different cache keys
2. @nki.jit and torch_neuronx.nki_jit() have different caching behavior

================================================================================
5. Recommended Fixes
================================================================================

1. Investigate caching mechanism
   - Check if nkilib generates stable HLO graphs
   - Compare @nki.jit and nki_jit() implementation differences

2. Possible solutions
   - Use torch_neuronx.nki_jit() wrapper for nkilib kernel
   - Use test framework's compile_klir_to_neff() for pre-compilation
   - Fix @nki.jit's caching mechanism

3. Benchmark best practices
   - Use neuron-profile to measure pure kernel execution time
   - Measure compile time and execution time separately
   - Ensure both kernels use the same measurement method

================================================================================
6. Fair Performance Comparison Results (Updated 2026-01-26)
================================================================================

Using NEURON_FRAMEWORK_DEBUG=1 + neuron-explorer capture method to measure actual kernel time

Method:
1. Set NEURON_FRAMEWORK_DEBUG=1 environment variable
2. Run kernel (NEFF file saved automatically)
3. Use neuron-explorer capture to profile NEFF

Results:
--------------------------------------------------------------------------------
Kernel       Config                              Total (μs)   Tensor (μs)  DMA (μs)
--------------------------------------------------------------------------------
neuronxcc    b=1, s=128, h=1024, i=512           39.36        14.66        19.80
nkilib       b=1, s=128, h=1024, i=512           43.22        15.22        21.08
neuronxcc    b=1, s=256, h=2048, i=1024          67.21        36.64        40.35
nkilib       b=1, s=256, h=2048, i=1024          85.26        57.09        53.12

Comparison:
- b=1, s=128, h=1024, i=512: neuronxcc is 1.10x faster than nkilib (39.36 vs 43.22 μs)
- b=1, s=256, h=2048, i=1024: neuronxcc is 1.27x faster than nkilib (67.21 vs 85.26 μs)

Conclusions:
- Under same configuration, neuronxcc's mlp_isa_kernel is 10-27% faster
- Both kernels have similar Tensor Engine time
- nkilib has slightly longer DMA time, possibly due to data layout differences

================================================================================
7. Profiling Method Summary
================================================================================

Method 1: KLIR Compilation + neuron-profile
----------------------------------------
Applicable for: Pure Python kernels that can be directly KLIR traced (e.g., nkilib)

Steps:
1. Use compile_klir_to_neff() to compile kernel
2. Save inputs as .bin files
3. neuron-profile capture --neff file.neff input_args
4. neuron-profile view --output-format=json to get detailed data

Method 2: NEURON_FRAMEWORK_DEBUG + neuron-explorer
----------------------------------------
Applicable for: Any kernel running through torch-xla (including Cython kernels)

Steps:
1. Set environment variables:
   export NEURON_FRAMEWORK_DEBUG=1
   export XLA_IR_DEBUG=1
   export XLA_HLO_DEBUG=1

2. Run kernel (NEFF saved automatically to current directory)
   python your_kernel_script.py

3. Profile NEFF:
   neuron-explorer capture -n <neff_file> -s profile.ntff --profile-nth-exec=2

4. Generate JSON report:
   neuron-profile view -n <neff_file> -s profile.ntff --output-format=json --output-file=profile.json

Note: neuron-explorer capture requires exclusive Neuron device access,
      so it must be run separately after kernel execution completes

================================================================================
8. Integration with nki-library
================================================================================

Copy the following two benchmark scripts to nki-library repository:
https://github.com/aws-neuron/nki-library/tree/main

Suggested directory structure:
```
nki-library/
└── benchmarks/
    └── mlp/
        ├── benchmark_mlp_step1_generate_neff.py
        ├── benchmark_mlp_step2_profile.py
        └── README.md
```

Steps:
1. Clone nki-library:
   git clone https://github.com/aws-neuron/nki-library.git
   cd nki-library

2. Create directory and copy files:
   mkdir -p benchmarks/mlp
   cp /path/to/benchmark_mlp_step1_generate_neff.py benchmarks/mlp/
   cp /path/to/benchmark_mlp_step2_profile.py benchmarks/mlp/

3. Commit and create PR:
   git checkout -b add-mlp-benchmark
   git add benchmarks/
   git commit -m "Add MLP kernel benchmark scripts"
   git push origin add-mlp-benchmark

================================================================================
9. Related Files
================================================================================

- benchmark_mlp_step1_generate_neff.py: Generate NEFF files (Step 1)
- benchmark_mlp_step2_profile.py: Profile NEFF files (Step 2)
- /tmp/mlp_benchmark_neffs/: NEFF and profile files output directory
